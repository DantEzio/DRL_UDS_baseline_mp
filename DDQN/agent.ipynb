{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003dbf8f-e577-4037-bca0-a346474268aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9480572e-5228-46e1-97e4-38d435434393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a9af5-a73c-49fc-8b54-13e3858756ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DDQN BC code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2f58-4bc0-47e0-8c4d-9e14c6d132cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.params['ddqn']:\n",
    "            #DDQN\n",
    "            #eval net\n",
    "            A_prev = self.s\n",
    "            for i in np.arange(self.params['evalnet_layer_A']):\n",
    "                A_prev=layers.Dense(self.params['evalnet_A'][i]['num'], activation='relu', name='evalnet_A'+str(i))(A_prev)\n",
    "            V_prev = self.s\n",
    "            for i in np.arange(self.params['evalnet_layer_V']):\n",
    "                V_prev=layers.Dense(self.params['evalnet_V'][i]['num'], activation='relu', name='evalnet_V'+str(i))(V_prev)\n",
    "            A_prev_avg = A_prev-tf.reduce_mean(A_prev)\n",
    "            self.eval_out = layers.Add()([V_prev,A_prev_avg])\n",
    "            \n",
    "            #target net\n",
    "            An_prev = self.s_next\n",
    "            for i in np.arange(self.params['targetnet_layer_A']):\n",
    "                An_prev=layers.Dense(self.params['targetnet_A'][i]['num'], activation='relu', name='targetnet_A'+str(i))(An_prev)\n",
    "            Vn_prev = self.s\n",
    "            for i in np.arange(self.params['targetnet_layer_V']):\n",
    "                Vn_prev=layers.Dense(self.params['targetnet_V'][i]['num'], activation='relu', name='targetnet_V'+str(i))(Vn_prev)\n",
    "            An_prev_avg = An_prev-tf.reduce_mean(An_prev)\n",
    "            self.target_out = layers.Add()([Vn_prev,An_prev_avg])\n",
    "        else:\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fd997-36ef-4b83-861c-510e24d95736",
   "metadata": {},
   "source": [
    "# DQN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d37aa1a-efd3-4c64-9932-3c541fe2fbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \n",
    "    def __init__(self,params,env):\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        self.params=params\n",
    "        self.memory_buffer = deque(maxlen=2000)\n",
    "        self.action_table=pd.read_excel('./action_table_of_DQN.xlsx').values[:,1:]\n",
    "        print('table shape: ',self.action_table.shape)\n",
    "        \n",
    "        self.model=_build_net()\n",
    "        self.target_model=_build_net()\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def _build_net(self):\n",
    "        self.s = layers.Input(shape=self.params['state_dim'],name='s_input')\n",
    "        self.s_next = layers.Input(shape=self.params['state_dim'],name='s_next_input')\n",
    "        self.q_target = layers.Input(shape=self.params['action_dim'],name='qt_input')\n",
    "        \n",
    "        #DQN\n",
    "        #eval net\n",
    "        V_prev = self.s\n",
    "        for i in np.arange(self.params['evalnet_layer_V']):\n",
    "            V_prev=layers.Dense(self.params['evalnet_V'][i]['num'], activation='relu', name='evalnet_V'+str(i))(V_prev)\n",
    "        \n",
    "        model=models.Model(inputs=[self.s,self.s_next,self.q_target],outputs=self.eval_out)\n",
    "        return model\n",
    "    \n",
    "    def choose_action(self,state,train_log):\n",
    "        #input state, output action\n",
    "        if train_log:\n",
    "            #epsilon greedy\n",
    "            pa = np.random.uniform()\n",
    "            if pa < self.params['epsilon']:\n",
    "                action_value = self.model.predict([state])\n",
    "                action = np.argmax(action_value)\n",
    "            else:\n",
    "                action = np.random.randit(0,self.params['action_dim'])\n",
    "        else:\n",
    "            action_value = self.model.predict([state])\n",
    "            action = np.argmax(action_value)\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        self.memory_buffer.append(item)\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "         # 从经验池中随机采样一个batch\n",
    "        data = random.sample(self.memory_buffer, batch)\n",
    "        # 生成Q_target。\n",
    "        states = np.array([d[0] for d in data])\n",
    "        next_states = np.array([d[3] for d in data])\n",
    "\n",
    "        y = self.model.predict(states)\n",
    "        q = self.target_model.predict(next_states)\n",
    "\n",
    "        for i, (_, action, reward, _, done) in enumerate(data):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(q[i])\n",
    "            y[i][action] = target\n",
    "        return states, y\n",
    "    \n",
    "    \n",
    "    def train(self,total_step):\n",
    "        #sampling and upgrading\n",
    "        for j in range(self.params['training_step']):\n",
    "            total_step=0\n",
    "            for i in range(self.params['num_rain']):\n",
    "                print('training step:',j,' sampling num:',i)\n",
    "                #Sampling: each rainfall represent one round of sampling\n",
    "                s = self.env.reset(self.RainData[i])\n",
    "                done, batch = False, 0\n",
    "                while not done:\n",
    "                    a = self.choose_action(s,True)\n",
    "                    action = self.action_table[a,:].tolist()\n",
    "                    snext,reward,done = self.env.step(action)\n",
    "                    self.remember(s, a, reward, snext, done)\n",
    "                    s = snext\n",
    "                    batch+=1\n",
    "                \n",
    "                #Upgrading: each rainfall for one round of upgrading\n",
    "                X, y = self.process_batch(batch)\n",
    "                loss = self.model.train_on_batch(X, y)\n",
    "                count += 1\n",
    "                # 减小egreedy的epsilon参数。\n",
    "                if self.epsilon >= self.epsilon_min:\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                # 固定次数更新target_model\n",
    "                if count != 0 and count % 20 == 0:\n",
    "                    self.target_model.set_weights(self.model.get_weights())\n",
    "                if i % 5 == 0:\n",
    "                    history['episode'].append(i)\n",
    "                    history['Episode_reward'].append(reward_sum)\n",
    "                    history['Loss'].append(loss)\n",
    "\n",
    "                    print('Episode: {} | Episode reward: {} | loss: {:.3f} | e:{:.2f}'.format(i, reward_sum, loss, self.epsilon))\n",
    "\n",
    "            self.model.save_weights('./model/dqn.h5')\n",
    "        return history\n",
    "    \n",
    "    def play(self):\n",
    "        observation = self.env.reset()\n",
    "\n",
    "        count = 0\n",
    "        reward_sum = 0\n",
    "        random_episodes = 0\n",
    "\n",
    "        while random_episodes < 10:\n",
    "            self.env.render()\n",
    "\n",
    "            x = observation.reshape(-1, 4)\n",
    "            q_values = self.model.predict(x)[0]\n",
    "            action = np.argmax(q_values)\n",
    "            observation, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            count += 1\n",
    "            reward_sum += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Reward for this episode was: {}, turns was: {}\".format(reward_sum, count))\n",
    "                random_episodes += 1\n",
    "                reward_sum = 0\n",
    "                count = 0\n",
    "                observation = self.env.reset()\n",
    "\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96d8d7f-62f1-4d93-8a10-69e24f0b65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SWMM_ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "750a9b64-e490-4fbe-a559-2d626bc7f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'state_dim':5,\n",
    "    'action_dim':8,\n",
    "    'evalnet_layer_A':3,\n",
    "    'evalnet_A':[{'num':30},{'num':30},{'num':30}],\n",
    "    'evalnet_layer_V':3,\n",
    "    'evalnet_V':[{'num':30},{'num':30},{'num':30}],\n",
    "    'targetnet_layer_A':3,\n",
    "    'targetnet_A':[{'num':30},{'num':30},{'num':30}],\n",
    "    'targetnet_layer_V':3,\n",
    "    'targetnet_V':[{'num':30},{'num':30},{'num':30}],\n",
    "    \n",
    "}\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c107b33-a613-4780-9de2-a5497b9d1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DQN(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd0d0b07-5de2-44fc-a5b7-e0dbccdc06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._build_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9efcc4d-56cd-484f-8b83-e6eb302d816e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
